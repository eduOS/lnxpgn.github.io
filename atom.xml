<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[竹里馆]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://lnxpgn.github.io//"/>
  <updated>2015-07-30T04:00:03.000Z</updated>
  <id>http://lnxpgn.github.io//</id>
  
  <author>
    <name><![CDATA[lnxpgn]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Using multiple spiders in a Scrapy project]]></title>
    <link href="http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project/"/>
    <id>http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project/</id>
    <published>2015-07-27T12:18:21.000Z</published>
    <updated>2015-07-30T04:00:03.000Z</updated>
    <content type="html"><![CDATA[<h2>Overview</h2>
<p>Different channel's structure in a websit are similar, sometimes we want to reuse source code and don't create a <a href="http://scrapy.org/" target="_blank" rel="external">Scrap</a> project per channel. This is a tutorial how to use multiple spiders in a Scrapy project.</p>
<h2>ENV</h2>
<p>Python: 2.7.5<br>
Scrapy: 0.24.2</p>
<h2>Tree-like directories of this tutorial project</h2>
<p>Source code in GitHub: <a href="http://xxx.com" target="_blank" rel="external">http://xxx.com</a></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scrapy_multiple_spiders</span><br><span class="line">├── commands</span><br><span class="line">│   ├── __init__<span class="class">.py</span></span><br><span class="line">│   └── crawl<span class="class">.py</span></span><br><span class="line">└── tutorial</span><br><span class="line">    ├── scrapy<span class="class">.cfg</span></span><br><span class="line">    └── tutorial</span><br><span class="line">        ├── __init__<span class="class">.py</span></span><br><span class="line">        ├── common_spider<span class="class">.py</span></span><br><span class="line">        ├── items<span class="class">.py</span></span><br><span class="line">        ├── pipelines<span class="class">.py</span></span><br><span class="line">        ├── settings<span class="class">.py</span></span><br><span class="line">        ├── spider_settings</span><br><span class="line">        │   ├── __init__<span class="class">.py</span></span><br><span class="line">        │   ├── spider1<span class="class">.py</span></span><br><span class="line">        │   └── spider2<span class="class">.py</span></span><br><span class="line">        └── spiders</span><br><span class="line">            ├── __init__<span class="class">.py</span></span><br><span class="line">            ├── spider1<span class="class">.py</span></span><br><span class="line">            └── spider2.py</span><br></pre></td></tr></table></figure>
<!-- mo-re -->
<h2>Custom project command</h2>
<p>In Scrapy we can add our custom project commands by using the COMMANDS_MODULE setting item in <em>settings.py</em>, we will custom the standard <em>&quot;crawl&quot;</em> command. When call <em>&quot;scrapy crawl &lt;spider name&gt;&quot;</em>, the <em>run()</em> function in <em>scrapy.commands.crawl.Command</em> is the entrance. Inherit <em>scrapy.commands.crawl.Command</em> and overwrite the <em>run()</em> function in our project's <em>commands.crawl.CustomCrawlCommand</em> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomCrawlCommand</span><span class="params">(Command)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError()</span><br><span class="line">        <span class="keyword">elif</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError(<span class="string">"running 'scrapy crawl' with more than one spider is no longer supported"</span>)</span><br><span class="line">        spname = args[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># added new code</span></span><br><span class="line">        spider_settings_path = self.settings.getdict(<span class="string">'SPIDER_SETTINGS'</span>, &#123;&#125;).get(spname, <span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">if</span> spider_settings_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.settings.setmodule(spider_settings_path, priority=<span class="string">'cmdline'</span>)</span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">                 </span><br><span class="line">        crawler = self.crawler_process.create_crawler()</span><br><span class="line">        spider = crawler.spiders.create(spname, **opts.spargs)</span><br><span class="line">        crawler.crawl(spider)</span><br><span class="line">        self.crawler_process.start()</span><br></pre></td></tr></table></figure>
<p>The commented part is new code, others are same as <em>scrapy.commands.crawl.Command.run()</em>. The Scrapy <em>settings</em> has four priorities: <em>default, command, project, cmdline</em>, the <em>cmdline</em> has a top priority, use it to overwrite default setting items which are in <em>settings.py</em>. <em>&quot;SPIDER_SETTINGS&quot;</em> is a setting item in settings.py, it is a directory including spiders' custom setting files.</p>
<h2>Create common spiders and settings</h2>
<p><em>tutorial.tutorial.common_spider.CommonSpider</em> is a spider which includes a normal parsing process for a website and some common functions. <em>settings.py</em> includes common setting items for all spiders, such as <em>LOG_LEVEL</em>, you can overwrite them in a spider custom setting file, such as <em>spider1.py</em> and <em>spider2.py</em> in <em>tutorial.tutorial.spider_settings</em> directory.</p>
<p>common_spider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommonSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">        This is a common spider, including common functions which child spiders can inherit or overwrite</span><br><span class="line">    """</span></span><br><span class="line">    name = <span class="string">''</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add "kwargs", otherwise can't run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(CommonSpider, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self._start_urls = []</span><br><span class="line">        self._start_urls.extend(settings.get(<span class="string">'START_URLS'</span>, []))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._start_urls:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'no urls to crawl'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self._start_urls:</span><br><span class="line">            <span class="comment"># must append these hosts, otherwise OffsiteMiddleware will filter them</span></span><br><span class="line">            parsed_url = urlparse.urlparse(url)</span><br><span class="line">            parsed_url.hostname <span class="keyword">and</span> self.allowed_domains.append(parsed_url.hostname)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># open('file name', 'a+') is different between OS X and Linux, </span></span><br><span class="line">            <span class="comment"># read an empty filter list from &lt;JOBDIR&gt;/requests.seen when launche the spider on OS X, </span></span><br><span class="line">            <span class="comment"># be careful "dont_filter"</span></span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=self.parse, method=<span class="string">'GET'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'response url: %s, status: %d'</span> % (response.url, response.status), INFO)</span><br></pre></td></tr></table></figure>
<p>settings.py</p>
<figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">COMMANDS_MODULE</span> = <span class="symbol">'commands'</span></span><br><span class="line"></span><br><span class="line"><span class="type">SPIDER_SETTINGS</span> = &#123;</span><br><span class="line">    <span class="symbol">'spider1'</span>: <span class="symbol">'tutorial</span>.spider_settings.spider1',</span><br><span class="line">    <span class="symbol">'spider2'</span>: <span class="symbol">'tutorial</span>.spider_settings.spider2',</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">LOG_LEVEL</span> = <span class="symbol">'INFO'</span></span><br></pre></td></tr></table></figure>
<h2>Create multiple spiders in a project</h2>
<h3>spider without custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider1.Spider1</em><br>
Spider1's setting file: spider1.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">LOG_FILE</span> = <span class="string">'spider1.log'</span></span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">'spider1_job'</span></span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">'http://www.bing.com/news'</span>]</span><br></pre></td></tr></table></figure>
<p>Spider1's source file: Spider1.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider1</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'spider1'</span></span><br></pre></td></tr></table></figure>
<h3>spider with custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider2.Spider2</em><br>
Spider2's setting file: spider2.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE = <span class="string">'spider2</span>.log'</span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">'spider2_job</span>'</span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">'http</span>:<span class="comment">//www.bing.com/knows']</span></span><br><span class="line"></span><br><span class="line">TITLE_PATH = <span class="string">'html</span> head title::text'</span><br></pre></td></tr></table></figure>
<p>Spider2's source file: Spider2.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.log <span class="keyword">import</span> INFO</span><br><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider2</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'spider2'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add "kwargs", otherwise can't run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(Spider2, self).__init__(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">        self._title_path = settings.get(<span class="string">'TITLE_PATH'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_other_info</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.css(self._title_path).extract()[<span class="number">0</span>]</span><br><span class="line">        self.log(<span class="string">'title: %s'</span> % title, INFO)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.parse_other_info(response)</span><br><span class="line"></span><br><span class="line">        super(Spider2, self).parse(response)</span><br></pre></td></tr></table></figure>
<h2>Run spiders</h2>
<ol>
<li>set <em>PYTHONPATH</em> to <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders&quot;</em></li>
<li>in <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders/tutorial&quot;</em>, call <em>scrapy crawl spider1</em> or <em>scrapy crawl spider2</em>, check log file <em>spider1.log</em> or <em>spider2.log</em></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<h2>Overview</h2>
<p>Different channel's structure in a websit are similar, sometimes we want to reuse source code and don't create a <a hre]]>
    </summary>
    
      <category term="Crawler" scheme="http://lnxpgn.github.io/tags/Crawler/"/>
    
      <category term="Python" scheme="http://lnxpgn.github.io/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://lnxpgn.github.io/tags/Scrapy/"/>
    
      <category term="爬虫" scheme="http://lnxpgn.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Using multiple spiders in a Scrapy project]]></title>
    <link href="http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project2/"/>
    <id>http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project2/</id>
    <published>2015-07-27T12:18:21.000Z</published>
    <updated>2015-07-30T03:46:40.000Z</updated>
    <content type="html"><![CDATA[<h2>Overview</h2>
<p>Different channel's structure in a websit are similar, sometimes we want to reuse source code and don't create a <a href="http://scrapy.org/" target="_blank" rel="external">Scrap</a> project per channel. This is a tutorial how to use multiple spiders in a Scrapy project.</p>
<h2>ENV</h2>
<p>Python: 2.7.5<br>
Scrapy: 0.24.2
中国</p>
<h2>Tree-like directories of this tutorial project</h2>
<p>Source code in GitHub: <a href="http://xxx.com" target="_blank" rel="external">http://xxx.com</a></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scrapy_multiple_spiders</span><br><span class="line">├── commands</span><br><span class="line">│   ├── __init__<span class="class">.py</span></span><br><span class="line">│   └── crawl<span class="class">.py</span></span><br><span class="line">└── tutorial</span><br><span class="line">    ├── scrapy<span class="class">.cfg</span></span><br><span class="line">    └── tutorial</span><br><span class="line">        ├── __init__<span class="class">.py</span></span><br><span class="line">        ├── common_spider<span class="class">.py</span></span><br><span class="line">        ├── items<span class="class">.py</span></span><br><span class="line">        ├── pipelines<span class="class">.py</span></span><br><span class="line">        ├── settings<span class="class">.py</span></span><br><span class="line">        ├── spider_settings</span><br><span class="line">        │   ├── __init__<span class="class">.py</span></span><br><span class="line">        │   ├── spider1<span class="class">.py</span></span><br><span class="line">        │   └── spider2<span class="class">.py</span></span><br><span class="line">        └── spiders</span><br><span class="line">            ├── __init__<span class="class">.py</span></span><br><span class="line">            ├── spider1<span class="class">.py</span></span><br><span class="line">            └── spider2.py</span><br></pre></td></tr></table></figure>
<h2>Custom project command</h2>
<p>In Scrapy we can add our custom project commands by using the COMMANDS_MODULE setting item in <em>settings.py</em>, we will custom the standard <em>&quot;crawl&quot;</em> command. When call <em>&quot;scrapy crawl &lt;spider name&gt;&quot;</em>, the <em>run()</em> function in <em>scrapy.commands.crawl.Command</em> is the entrance. Inherit <em>scrapy.commands.crawl.Command</em> and overwrite the <em>run()</em> function in our project's <em>commands.crawl.CustomCrawlCommand</em> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomCrawlCommand</span><span class="params">(Command)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError()</span><br><span class="line">        <span class="keyword">elif</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError(<span class="string">"running 'scrapy crawl' with more than one spider is no longer supported"</span>)</span><br><span class="line">        spname = args[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># added new code</span></span><br><span class="line">        spider_settings_path = self.settings.getdict(<span class="string">'SPIDER_SETTINGS'</span>, &#123;&#125;).get(spname, <span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">if</span> spider_settings_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.settings.setmodule(spider_settings_path, priority=<span class="string">'cmdline'</span>)</span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">                 </span><br><span class="line">        crawler = self.crawler_process.create_crawler()</span><br><span class="line">        spider = crawler.spiders.create(spname, **opts.spargs)</span><br><span class="line">        crawler.crawl(spider)</span><br><span class="line">        self.crawler_process.start()</span><br></pre></td></tr></table></figure>
<p>The commented part is new code, others are same as <em>scrapy.commands.crawl.Command.run()</em>. The Scrapy <em>settings</em> has four priorities: <em>default, command, project, cmdline</em>, the <em>cmdline</em> has a top priority, use it to overwrite default setting items which are in <em>settings.py</em>. <em>&quot;SPIDER_SETTINGS&quot;</em> is a setting item in settings.py, it is a directory including spiders' custom setting files.</p>
<a id="more"></a>
<h2>Create common spiders and settings</h2>
<p><em>tutorial.tutorial.common_spider.CommonSpider</em> is a spider which includes a normal parsing process for a website and some common functions. <em>settings.py</em> includes common setting items for all spiders, such as <em>LOG_LEVEL</em>, you can overwrite them in a spider custom setting file, such as <em>spider1.py</em> and <em>spider2.py</em> in <em>tutorial.tutorial.spider_settings</em> directory.</p>
<p>common_spider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommonSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">        This is a common spider, including common functions which child spiders can inherit or overwrite</span><br><span class="line">    """</span></span><br><span class="line">    name = <span class="string">''</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add "kwargs", otherwise can't run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(CommonSpider, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self._start_urls = []</span><br><span class="line">        self._start_urls.extend(settings.get(<span class="string">'START_URLS'</span>, []))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._start_urls:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'no urls to crawl'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self._start_urls:</span><br><span class="line">            <span class="comment"># must append these hosts, otherwise OffsiteMiddleware will filter them</span></span><br><span class="line">            parsed_url = urlparse.urlparse(url)</span><br><span class="line">            parsed_url.hostname <span class="keyword">and</span> self.allowed_domains.append(parsed_url.hostname)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># open('file name', 'a+') is different between OS X and Linux, </span></span><br><span class="line">            <span class="comment"># read an empty filter list from &lt;JOBDIR&gt;/requests.seen when launche the spider on OS X, </span></span><br><span class="line">            <span class="comment"># be careful "dont_filter"</span></span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=self.parse, method=<span class="string">'GET'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'response url: %s, status: %d'</span> % (response.url, response.status), INFO)</span><br></pre></td></tr></table></figure>
<p>settings.py</p>
<figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">COMMANDS_MODULE</span> = <span class="symbol">'commands'</span></span><br><span class="line"></span><br><span class="line"><span class="type">SPIDER_SETTINGS</span> = &#123;</span><br><span class="line">    <span class="symbol">'spider1'</span>: <span class="symbol">'tutorial</span>.spider_settings.spider1',</span><br><span class="line">    <span class="symbol">'spider2'</span>: <span class="symbol">'tutorial</span>.spider_settings.spider2',</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">LOG_LEVEL</span> = <span class="symbol">'INFO'</span></span><br></pre></td></tr></table></figure>
<h2>Create multiple spiders in a project</h2>
<h3>spider without custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider1.Spider1</em><br>
Spider1's setting file: spider1.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">LOG_FILE</span> = <span class="string">'spider1.log'</span></span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">'spider1_job'</span></span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">'http://www.bing.com/news'</span>]</span><br></pre></td></tr></table></figure>
<p>Spider1's source file: Spider1.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider1</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'spider1'</span></span><br></pre></td></tr></table></figure>
<h3>spider with custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider2.Spider2</em><br>
Spider2's setting file: spider2.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE = <span class="string">'spider2</span>.log'</span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">'spider2_job</span>'</span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">'http</span>:<span class="comment">//www.bing.com/knows']</span></span><br><span class="line"></span><br><span class="line">TITLE_PATH = <span class="string">'html</span> head title::text'</span><br></pre></td></tr></table></figure>
<p>Spider2's source file: Spider2.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.log <span class="keyword">import</span> INFO</span><br><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider2</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'spider2'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add "kwargs", otherwise can't run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(Spider2, self).__init__(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">        self._title_path = settings.get(<span class="string">'TITLE_PATH'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_other_info</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.css(self._title_path).extract()[<span class="number">0</span>]</span><br><span class="line">        self.log(<span class="string">'title: %s'</span> % title, INFO)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.parse_other_info(response)</span><br><span class="line"></span><br><span class="line">        super(Spider2, self).parse(response)</span><br></pre></td></tr></table></figure>
<h2>Run spiders</h2>
<ol>
<li>set <em>PYTHONPATH</em> to <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders&quot;</em></li>
<li>in <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders/tutorial&quot;</em>, call <em>scrapy crawl spider1</em> or <em>scrapy crawl spider2</em>, check log file <em>spider1.log</em> or <em>spider2.log</em></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<h2>Overview</h2>
<p>Different channel's structure in a websit are similar, sometimes we want to reuse source code and don't create a <a href="http://scrapy.org/">Scrap</a> project per channel. This is a tutorial how to use multiple spiders in a Scrapy project.</p>
<h2>ENV</h2>
<p>Python: 2.7.5<br>
Scrapy: 0.24.2
中国</p>
<h2>Tree-like directories of this tutorial project</h2>
<p>Source code in GitHub: <a href="http://xxx.com">http://xxx.com</a></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scrapy_multiple_spiders</span><br><span class="line">├── commands</span><br><span class="line">│   ├── __init__<span class="class">.py</span></span><br><span class="line">│   └── crawl<span class="class">.py</span></span><br><span class="line">└── tutorial</span><br><span class="line">    ├── scrapy<span class="class">.cfg</span></span><br><span class="line">    └── tutorial</span><br><span class="line">        ├── __init__<span class="class">.py</span></span><br><span class="line">        ├── common_spider<span class="class">.py</span></span><br><span class="line">        ├── items<span class="class">.py</span></span><br><span class="line">        ├── pipelines<span class="class">.py</span></span><br><span class="line">        ├── settings<span class="class">.py</span></span><br><span class="line">        ├── spider_settings</span><br><span class="line">        │   ├── __init__<span class="class">.py</span></span><br><span class="line">        │   ├── spider1<span class="class">.py</span></span><br><span class="line">        │   └── spider2<span class="class">.py</span></span><br><span class="line">        └── spiders</span><br><span class="line">            ├── __init__<span class="class">.py</span></span><br><span class="line">            ├── spider1<span class="class">.py</span></span><br><span class="line">            └── spider2.py</span><br></pre></td></tr></table></figure>
<h2>Custom project command</h2>
<p>In Scrapy we can add our custom project commands by using the COMMANDS_MODULE setting item in <em>settings.py</em>, we will custom the standard <em>&quot;crawl&quot;</em> command. When call <em>&quot;scrapy crawl &lt;spider name&gt;&quot;</em>, the <em>run()</em> function in <em>scrapy.commands.crawl.Command</em> is the entrance. Inherit <em>scrapy.commands.crawl.Command</em> and overwrite the <em>run()</em> function in our project's <em>commands.crawl.CustomCrawlCommand</em> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomCrawlCommand</span><span class="params">(Command)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError()</span><br><span class="line">        <span class="keyword">elif</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError(<span class="string">"running 'scrapy crawl' with more than one spider is no longer supported"</span>)</span><br><span class="line">        spname = args[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># added new code</span></span><br><span class="line">        spider_settings_path = self.settings.getdict(<span class="string">'SPIDER_SETTINGS'</span>, &#123;&#125;).get(spname, <span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">if</span> spider_settings_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.settings.setmodule(spider_settings_path, priority=<span class="string">'cmdline'</span>)</span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">                 </span><br><span class="line">        crawler = self.crawler_process.create_crawler()</span><br><span class="line">        spider = crawler.spiders.create(spname, **opts.spargs)</span><br><span class="line">        crawler.crawl(spider)</span><br><span class="line">        self.crawler_process.start()</span><br></pre></td></tr></table></figure>
<p>The commented part is new code, others are same as <em>scrapy.commands.crawl.Command.run()</em>. The Scrapy <em>settings</em> has four priorities: <em>default, command, project, cmdline</em>, the <em>cmdline</em> has a top priority, use it to overwrite default setting items which are in <em>settings.py</em>. <em>&quot;SPIDER_SETTINGS&quot;</em> is a setting item in settings.py, it is a directory including spiders' custom setting files.</p>]]>
    
    </summary>
    
      <category term="Crawler" scheme="http://lnxpgn.github.io/tags/Crawler/"/>
    
      <category term="Python" scheme="http://lnxpgn.github.io/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://lnxpgn.github.io/tags/Scrapy/"/>
    
      <category term="爬虫" scheme="http://lnxpgn.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>