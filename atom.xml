<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[竹里馆]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://lnxpgn.github.io//"/>
  <updated>2015-08-01T08:20:06.000Z</updated>
  <id>http://lnxpgn.github.io//</id>
  
  <author>
    <name><![CDATA[lnxpgn]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Hexo + Markdown issues]]></title>
    <link href="http://lnxpgn.github.io/2015/08/01/hexo-markdown-issues/"/>
    <id>http://lnxpgn.github.io/2015/08/01/hexo-markdown-issues/</id>
    <published>2015-08-01T08:19:03.000Z</published>
    <updated>2015-08-01T08:20:06.000Z</updated>
    <content type="html"><![CDATA[<p>##Backslash escapes
Hexo installs a plugin <em>hexo-renderer-marked</em> as the default Markdown renderer, but the plugin generates literal characters which are not what you want if have a backslash before these characters, such as <em>&quot;scrapy crawl \&lt;spider name\&gt;&quot;</em> becomes <em>&quot;scrapy crawl \&quot;</em>. To fix this issue by replacing <em>hexo-renderer-marked</em> with <em>hexo-renderer-markdown-it</em>.</p>
<p>##&quot;Read More&quot; link
Normally, we insert <em>&quot;&lt;!-- more --&gt;&quot;</em> into a post to display its summary, but don&apos;t work if using <em>hexo-renderer-markdown-it</em> with default configuration, <em>hexo-renderer-markdown-it</em> escapes HTML. To fix this issue by appending</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">markdown</span>:</span><br><span class="line">  <span class="attribute">render</span>:</span><br><span class="line">    <span class="attribute">html</span>: true</span><br></pre></td></tr></table></figure>
<p>to <em>Hexo</em>&apos;s <em>_config.yml</em>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>##Backslash escapes
Hexo installs a plugin <em>hexo-renderer-marked</em> as the default Markdown renderer, but the plugin generates liter]]>
    </summary>
    
      <category term="Hexo" scheme="http://lnxpgn.github.io/tags/Hexo/"/>
    
      <category term="Markdown" scheme="http://lnxpgn.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Using multiple spiders in a Scrapy project]]></title>
    <link href="http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project/"/>
    <id>http://lnxpgn.github.io/2015/07/27/using-multiple-spiders-in-a-scrapy-project/</id>
    <published>2015-07-27T12:18:21.000Z</published>
    <updated>2015-08-01T05:14:09.000Z</updated>
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2>
<p>Different channel&apos;s structure in a websit are similar, sometimes we want to reuse source code and don&apos;t create a <a href="http://scrapy.org/" target="_blank" rel="external">Scrap</a> project per channel. This is a tutorial how to use multiple spiders in a Scrapy project.</p>
<h2 id="env">ENV</h2>
<p>Python: 2.7.5<br>
Scrapy: 0.24.2</p>
<h2 id="tree-like-directories-of-this-tutorial-project">Tree-like directories of this tutorial project</h2>
<p>Source code in GitHub: <a href="https://github.com/lnxpgn/scrapy_multiple_spiders" target="_blank" rel="external">scrapy_multiple_spiders</a></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scrapy_multiple_spiders</span><br><span class="line">&#x251C;&#x2500;&#x2500; commands</span><br><span class="line">&#x2502;&#xA0;&#xA0; &#x251C;&#x2500;&#x2500; __init__<span class="class">.py</span></span><br><span class="line">&#x2502;&#xA0;&#xA0; &#x2514;&#x2500;&#x2500; crawl<span class="class">.py</span></span><br><span class="line">&#x2514;&#x2500;&#x2500; tutorial</span><br><span class="line">    &#x251C;&#x2500;&#x2500; scrapy<span class="class">.cfg</span></span><br><span class="line">    &#x2514;&#x2500;&#x2500; tutorial</span><br><span class="line">        &#x251C;&#x2500;&#x2500; __init__<span class="class">.py</span></span><br><span class="line">        &#x251C;&#x2500;&#x2500; common_spider<span class="class">.py</span></span><br><span class="line">        &#x251C;&#x2500;&#x2500; items<span class="class">.py</span></span><br><span class="line">        &#x251C;&#x2500;&#x2500; pipelines<span class="class">.py</span></span><br><span class="line">        &#x251C;&#x2500;&#x2500; settings<span class="class">.py</span></span><br><span class="line">        &#x251C;&#x2500;&#x2500; spider_settings</span><br><span class="line">        &#x2502;&#xA0;&#xA0; &#x251C;&#x2500;&#x2500; __init__<span class="class">.py</span></span><br><span class="line">        &#x2502;&#xA0;&#xA0; &#x251C;&#x2500;&#x2500; spider1<span class="class">.py</span></span><br><span class="line">        &#x2502;&#xA0;&#xA0; &#x2514;&#x2500;&#x2500; spider2<span class="class">.py</span></span><br><span class="line">        &#x2514;&#x2500;&#x2500; spiders</span><br><span class="line">            &#x251C;&#x2500;&#x2500; __init__<span class="class">.py</span></span><br><span class="line">            &#x251C;&#x2500;&#x2500; spider1<span class="class">.py</span></span><br><span class="line">            &#x2514;&#x2500;&#x2500; spider2.py</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="custom-project-command">Custom project command</h2>
<p>In Scrapy we can add our custom project commands by using the COMMANDS_MODULE setting item in <em>settings.py</em>, we will custom the standard <em>&quot;crawl&quot;</em> command. When call <em>&quot;scrapy crawl &lt;spider name&gt;&quot;</em>, the <em>run()</em> function in <em>scrapy.commands.crawl.Command</em> is the entrance. Inherit <em>scrapy.commands.crawl.Command</em> and overwrite the <em>run()</em> function in our project&apos;s <em>commands.crawl.CustomCrawlCommand</em> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomCrawlCommand</span><span class="params">(Command)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError()</span><br><span class="line">        <span class="keyword">elif</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> UsageError(<span class="string">&quot;running &apos;scrapy crawl&apos; with more than one spider is no longer supported&quot;</span>)</span><br><span class="line">        spname = args[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># added new code</span></span><br><span class="line">        spider_settings_path = self.settings.getdict(<span class="string">&apos;SPIDER_SETTINGS&apos;</span>, {}).get(spname, <span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">if</span> spider_settings_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.settings.setmodule(spider_settings_path, priority=<span class="string">&apos;cmdline&apos;</span>)</span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">                 </span><br><span class="line">        crawler = self.crawler_process.create_crawler()</span><br><span class="line">        spider = crawler.spiders.create(spname, **opts.spargs)</span><br><span class="line">        crawler.crawl(spider)</span><br><span class="line">        self.crawler_process.start()</span><br></pre></td></tr></table></figure>
<p>The commented part is new code, others are same as run() function in <em>scrapy.commands.crawl.Command</em> class. The Scrapy <em>settings</em> has four priorities: <em>default, command, project, cmdline</em>, the <em>cmdline</em> has a top priority, use it to overwrite default setting items which are in <em>settings.py</em>. <em>&quot;SPIDER_SETTINGS&quot;</em> is a setting item in settings.py, it is a dictionary, the key is a spider name, the value is the spider&apos;s custom setting file name.</p>
<h2 id="create-common-spiders-and-settings">Create common spiders and settings</h2>
<p><em>tutorial.tutorial.common_spider.CommonSpider</em> is a spider which includes a normal parsing process for a website and some common functions. <em>settings.py</em> includes common setting items for all spiders, such as <em>LOG_LEVEL</em>, you can overwrite them in a spider custom setting file, such as <em>spider1.py</em> and <em>spider2.py</em> in <em>tutorial.tutorial.spider_settings</em> directory.</p>
<p>common_spider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommonSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span><br><span class="line">        This is a common spider, including common functions which child spiders can inherit or overwrite</span><br><span class="line">    &quot;&quot;&quot;</span></span><br><span class="line">    name = <span class="string">&apos;&apos;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add &quot;kwargs&quot;, otherwise can&apos;t run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(CommonSpider, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self._start_urls = []</span><br><span class="line">        self._start_urls.extend(settings.get(<span class="string">&apos;START_URLS&apos;</span>, []))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._start_urls:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&apos;no urls to crawl&apos;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="decorator">@classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self._start_urls:</span><br><span class="line">            <span class="comment"># must append these hosts, otherwise OffsiteMiddleware will filter them</span></span><br><span class="line">            parsed_url = urlparse.urlparse(url)</span><br><span class="line">            parsed_url.hostname <span class="keyword">and</span> self.allowed_domains.append(parsed_url.hostname)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># open(&apos;file name&apos;, &apos;a+&apos;) is different between OS X and Linux, </span></span><br><span class="line">            <span class="comment"># read an empty filter list from &lt;JOBDIR&gt;/requests.seen when launche the spider on OS X, </span></span><br><span class="line">            <span class="comment"># be careful &quot;dont_filter&quot;</span></span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=self.parse, method=<span class="string">&apos;GET&apos;</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">&apos;response url: %s, status: %d&apos;</span> % (response.url, response.status), INFO)</span><br></pre></td></tr></table></figure>
<p>settings.py</p>
<figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">COMMANDS_MODULE</span> = <span class="symbol">&apos;commands&apos;</span></span><br><span class="line"></span><br><span class="line"><span class="type">SPIDER_SETTINGS</span> = {</span><br><span class="line">    <span class="symbol">&apos;spider1&apos;</span>: <span class="symbol">&apos;tutorial</span>.spider_settings.spider1&apos;,</span><br><span class="line">    <span class="symbol">&apos;spider2&apos;</span>: <span class="symbol">&apos;tutorial</span>.spider_settings.spider2&apos;,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">LOG_LEVEL</span> = <span class="symbol">&apos;INFO&apos;</span></span><br></pre></td></tr></table></figure>
<h2 id="create-multiple-spiders-in-a-project">Create multiple spiders in a project</h2>
<h3 id="spider-without-custom-parsing-process">spider without custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider1.Spider1</em><br>
Spider1&apos;s setting file: spider1.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">LOG_FILE</span> = <span class="string">&apos;spider1.log&apos;</span></span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">&apos;spider1_job&apos;</span></span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">&apos;http://www.bing.com/news&apos;</span>]</span><br></pre></td></tr></table></figure>
<p>Spider1&apos;s source file: Spider1.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider1</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">&apos;spider1&apos;</span></span><br></pre></td></tr></table></figure>
<h3 id="spider-with-custom-parsing-process">spider with custom parsing process</h3>
<p>like <em>tutorial.tutorial.spiders.spider2.Spider2</em><br>
Spider2&apos;s setting file: spider2.py (in <em>&quot;spider_settings&quot;</em> directory)</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE = <span class="string">&apos;spider2</span>.log&apos;</span><br><span class="line"></span><br><span class="line">JOBDIR=<span class="string">&apos;spider2_job</span>&apos;</span><br><span class="line"></span><br><span class="line">START_URLS = [<span class="string">&apos;http</span>:<span class="comment">//www.bing.com/knows&apos;]</span></span><br><span class="line"></span><br><span class="line">TITLE_PATH = <span class="string">&apos;html</span> head title::text&apos;</span><br></pre></td></tr></table></figure>
<p>Spider2&apos;s source file: Spider2.py (in <em>&quot;spiders&quot;</em> directory)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.log <span class="keyword">import</span> INFO</span><br><span class="line"><span class="keyword">from</span> ..common_spider <span class="keyword">import</span> CommonSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider2</span><span class="params">(CommonSpider)</span>:</span></span><br><span class="line">    name = <span class="string">&apos;spider2&apos;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># must add &quot;kwargs&quot;, otherwise can&apos;t run in scrapyd</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings, **kwargs)</span>:</span></span><br><span class="line">        super(Spider2, self).__init__(settings, **kwargs)</span><br><span class="line"></span><br><span class="line">        self._title_path = settings.get(<span class="string">&apos;TITLE_PATH&apos;</span>, <span class="string">&apos;&apos;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_other_info</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.css(self._title_path).extract()[<span class="number">0</span>]</span><br><span class="line">        self.log(<span class="string">&apos;title: %s&apos;</span> % title, INFO)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.parse_other_info(response)</span><br><span class="line"></span><br><span class="line">        super(Spider2, self).parse(response)</span><br></pre></td></tr></table></figure>
<h2 id="run-spiders">Run spiders</h2>
<ol>
<li>set <em>PYTHONPATH</em> to <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders&quot;</em></li>
<li>in <em>&quot;/&lt;path&gt;/scrapy_multiple_spiders/tutorial&quot;</em>, call <em>scrapy crawl spider1</em> or <em>scrapy crawl spider2</em>, check log file <em>spider1.log</em> or <em>spider2.log</em></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<h2>Overview</h2>
<p>Different channel's structure in a websit are similar, sometimes we want to reuse source code and don't create a <a href="http://scrapy.org/">Scrap</a> project per channel. This is a tutorial how to use multiple spiders in a Scrapy project.</p>
<h2>ENV</h2>
<p>Python: 2.7.5<br>
Scrapy: 0.24.2</p>
<h2>Tree-like directories of this tutorial project</h2>
<p>Source code in GitHub: <a href="https://github.com/lnxpgn/scrapy_multiple_spiders">scrapy_multiple_spiders</a></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scrapy_multiple_spiders</span><br><span class="line">├── commands</span><br><span class="line">│   ├── __init__<span class="class">.py</span></span><br><span class="line">│   └── crawl<span class="class">.py</span></span><br><span class="line">└── tutorial</span><br><span class="line">    ├── scrapy<span class="class">.cfg</span></span><br><span class="line">    └── tutorial</span><br><span class="line">        ├── __init__<span class="class">.py</span></span><br><span class="line">        ├── common_spider<span class="class">.py</span></span><br><span class="line">        ├── items<span class="class">.py</span></span><br><span class="line">        ├── pipelines<span class="class">.py</span></span><br><span class="line">        ├── settings<span class="class">.py</span></span><br><span class="line">        ├── spider_settings</span><br><span class="line">        │   ├── __init__<span class="class">.py</span></span><br><span class="line">        │   ├── spider1<span class="class">.py</span></span><br><span class="line">        │   └── spider2<span class="class">.py</span></span><br><span class="line">        └── spiders</span><br><span class="line">            ├── __init__<span class="class">.py</span></span><br><span class="line">            ├── spider1<span class="class">.py</span></span><br><span class="line">            └── spider2.py</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="Crawler" scheme="http://lnxpgn.github.io/tags/Crawler/"/>
    
      <category term="Python" scheme="http://lnxpgn.github.io/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://lnxpgn.github.io/tags/Scrapy/"/>
    
      <category term="爬虫" scheme="http://lnxpgn.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>